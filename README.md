# Continual learning
## Future work
* LifeLonger: A Benchmark for Continual Disease Classification [[paper]](https://arxiv.org/pdf/2204.05737.pdf) [[code]](https://github.com/mmderakhshani/lifelonger)

## Present
### Focusing
* Flattening Sharpness for Dynamic Gradient Projection Memory Benefits Continual Learning [[paper]](https://openreview.net/pdf?id=q1eCa1kMfDd)
* CPR: Classifier-Projection Regularization for Continual Learning [[paper]](https://arxiv.org/abs/2006.07326) [[code]](https://github.com/csm9493/CPR_CL)
* Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima [[paper]](https://openreview.net/pdf?id=ALvt7nXa2q)
* Class-Incremental Continual Learning into the eXtended DER-verse [[paper]](https://arxiv.org/abs/2201.00766) [[code]](https://github.com/aimagelab/mammoth)
### Other
* DualNet: Continual Learning, Fast and Slow [[paper]](https://arxiv.org/abs/2110.00175) [[code]](https://github.com/phquang/DualNet)
* Contextual Transformation Networks for Online Continual Learning [[paper]](https://openreview.net/forum?id=zx_uX-BO7CH)
* Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System [[paper]](https://openreview.net/pdf?id=uxxFrDwrE7Y) [[code]](https://github.com/NeurAI-Lab/CLS-ER)
* Overcoming catastrophic forgetting in neural networks [[paper]](https://arxiv.org/abs/1612.00796) [[code]](https://github.com/ariseff/overcoming-catastrophic) [[code]](https://github.com/stokesj/EWC)
* iCaRL: Incremental Classifier and Representation Learning [[paper]](https://arxiv.org/abs/1611.07725) [[code]](https://github.com/srebuffi/iCaRL)

## Done
#### Paper
* Dark Experience for General Continual Learning: a Strong, Simple Baseline [[paper]](https://paperswithcode.com/paper/dark-experience-for-general-continual) [[code]](https://github.com/aimagelab/mammoth)
* Task Agnostic Representation Consolidation: a Self-supervised based Continual Learning Approach [[paper]](https://arxiv.org/abs/2207.06267) [[code]](https://github.com/neurai-lab/tarc)
* Complementary Learning for Overcoming Catastrophic Forgetting Using Experience Replay [[paper]](https://arxiv.org/abs/1903.04566)
* On Tiny Episodic Memories in Continual Learning [[paper]](https://arxiv.org/abs/1902.10486) [[code]](https://github.com/facebookresearch/agem)
* Efficient Lifelong Learning with A-GEM [[paper]](https://openreview.net/forum?id=Hkf2_sC5FX) [[code]](https://github.com/facebookresearch/agem)
* Gradient Episodic Memory for Continual Learning [[paper]](https://arxiv.org/abs/1706.08840) [[code]](https://github.com/facebookresearch/GradientEpisodicMemory)
* Efficient Continual Learning with Modular Networks and Task-Driven Priors [[paper]](https://arxiv.org/abs/2012.12631) [[code]](https://github.com/TomVeniat/MNTDP) [[code]](https://github.com/facebookresearch/CTrLBenchmark)

#### Thesis
* Continual Learning with Deep Architectures [[paper]](https://icml.cc/Conferences/2021/ScheduleMultitrack?event=10833)
* Continual Learning in Neural Networks [[paper]](https://arxiv.org/abs/1910.02718)
* Continual Learning for Efficient Machine Learning [[paper]](https://ora.ox.ac.uk/objects/uuid:7a3e5c33-864f-4cfe-8b80-e85cbf651946/files/ddf65v7983)

#### Other
* Distilling the Knowledge in a Neural Network [[paper]](https://arxiv.org/abs/1503.02531)
* Obtaining Well Calibrated Probabilities Using Bayesian Binning [[paper]](https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf)
