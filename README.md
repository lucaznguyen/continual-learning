# Continual learning

## This Week (7/8/2023 - 14/8/2023) (Done)
* FearNet: Brain-Inspired Model for Incremental Learning [[paper](https://openreview.net/pdf?id=SJ1Xmf-Rb)]

## Done
### 2022
* Dark Experience for General Continual Learning: a Strong, Simple Baseline [[paper](https://paperswithcode.com/paper/dark-experience-for-general-continual)] [[code](https://github.com/aimagelab/mammoth)]
* Task Agnostic Representation Consolidation: a Self-supervised based Continual Learning Approach [[paper](https://arxiv.org/abs/2207.06267)] [[code](https://github.com/neurai-lab/tarc)]
### 2019
* Efficient Lifelong Learning with A-GEM [[paper](https://openreview.net/forum?id=Hkf2_sC5FX)] [[code](https://github.com/facebookresearch/agem)]
* Efficient Continual Learning with Modular Networks and Task-Driven Priors [[paper](https://arxiv.org/abs/2012.12631)] [[code](https://github.com/TomVeniat/MNTDP)] [[code](https://github.com/facebookresearch/CTrLBenchmark)]
### 2018
* Exemplar-Supported Generative Reproduction for Class Incremental Learning [[slide](https://drive.google.com/file/d/1rwfaOYT06SHMSvi0MNvgciGILl6Xe1pw/view?usp=sharing)] [[paper](http://bmvc2018.org/contents/papers/0325.pdf)] [[code](https://github.com/TonyPod/ESGR)]
* End-to-End Incremental Learning [[paper](https://arxiv.org/abs/1807.09536)][[code](https://github.com/fmcp/EndToEndIncrementalLearning)]
### 2017
* Overcoming catastrophic forgetting in neural networks (EWC) [[slide](https://drive.google.com/file/d/1m0Vr8PKcyQB0fqOy3M8amKMVQuwTxJnB/view?usp=sharing)] [[paper](https://arxiv.org/abs/1612.00796)] [[code](https://github.com/ariseff/overcoming-catastrophic) [[code](https://github.com/stokesj/EWC)]
* Continual Learning Through Synaptic Intelligence [[slide](https://drive.google.com/file/d/12lhqVSlkHIP725C5NfKN-ZCuH1A2GQlS/view?usp=sharing)] [[paper](http://proceedings.mlr.press/v70/zenke17a.html)] [[code](https://github.com/ganguli-lab/pathint)]
* Gradient Episodic Memory for Continual Learning [[slide](https://drive.google.com/file/d/1715DA4BPWOGximi03kOAIRXatEoGdCWo/view?usp=sharing)] [[paper](https://arxiv.org/abs/1706.08840)] [[code](https://github.com/facebookresearch/GradientEpisodicMemory)]
* iCaRL: Incremental Classifier and Representation Learning [[slide](https://drive.google.com/file/d/10Ed3K1Kd8bKee_4S6ZX2m30LgTTxVoS1/view?usp=sharing)] [[paper](https://arxiv.org/abs/1611.07725)] [[code](https://github.com/srebuffi/iCaRL)]
### 2016
* Learning without forgetting [[slide](https://drive.google.com/file/d/1oFjS8bDEHVrO4TStTdndTk87Fr6_WdZb/view?usp=sharing)] [[paper](https://link.springer.com/chapter/10.1007/978-3-319-46493-0_37)] [[code](https://github.com/lizhitwo/LearningWithoutForgetting)]

### Thesis
* Continual Learning with Deep Architectures [[paper](https://icml.cc/Conferences/2021/ScheduleMultitrack?event=10833)]
* Continual Learning in Neural Networks [[paper](https://arxiv.org/abs/1910.02718)]
* Continual Learning for Efficient Machine Learning [[paper](https://ora.ox.ac.uk/objects/uuid:7a3e5c33-864f-4cfe-8b80-e85cbf651946/files/ddf65v7983)]

### Other
* Distilling the Knowledge in a Neural Network [[paper]](https://arxiv.org/abs/1503.02531)
* Obtaining Well Calibrated Probabilities Using Bayesian Binning [[paper]](https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf)
